# ğŸ•¸ï¸ Python Scraper â€“ Newegg Product Data Collection

This project is a Python-based web scraper that collects product information from [Newegg](https://www.newegg.com/) using BeautifulSoup and `requests`. It extracts key product details and exports them to a CSV file.

---

## ğŸš€ Features

- Scrapes product title, description, pricing, rating, seller, and main image
- Dynamically configured via `scraper_config.json`
- Built-in polite delays to avoid rate-limiting
- Retry logic for failed requests
- TQDM progress bar
- Clean project structure with unit tests

---

## ğŸ“ Project Structure
<pre>
python-scraper/
â”‚
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ README.md                   # Project documentation
â”œâ”€â”€ pytest.ini                  # Pytest configuration
â”œâ”€â”€ requirements.txt            # Dependencies list
â”œâ”€â”€ scraper_config.json         # Scraper settings (target URL, delay, etc.)
â”‚
â”œâ”€â”€ exported_data/              # Output CSV files go here
â”‚
â”œâ”€â”€ logs/                       # Logs from the scraper
â”‚   â””â”€â”€ scraper_errors.log
â”‚
â”œâ”€â”€ src/                        # Main source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Loads and parses scraper_config.json
â”‚   â”œâ”€â”€ export.py               # CSV export logic
â”‚   â”œâ”€â”€ main.py                 # Entry point to run the scraper
â”‚   â”œâ”€â”€ product_scraper.py      # Functions for scraping product data
â”‚   â””â”€â”€ utils.py                # Utility functions (e.g., polite_delay)
â”‚
â”œâ”€â”€ tests/                      # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_scraper.py         # Tests for product scraping logic
â”‚   â””â”€â”€ test_utils.py           # Tests for delay utility
â”‚
â””â”€â”€ venv/                       # (Local virtual environment â€“ ignored by Git)
</pre>



## âš™ï¸ Configuration (`scraper_config.json`)

```json
{
  "base_url": "https://www.newegg.com/",
  "search_query": "graphics+card",
  "max_products": 500,
  "output_file": "exported_data/newegg_products_{datetime}.csv",
  "user_agent": "Mozilla/5.0 (...) Chrome/113.0.0.0 Safari/537.36",
  "delay_between_requests": 1.5
}
```

- `max_products`: Number of products to scrape
- `output_file`: Path with optional `{datetime}` placeholder
- `delay_between_requests`: Seconds to wait between requests
- `user_agent`: User agent to mimic real browser


---

## ğŸ›  Installation

```bash
git clone https://github.com/zojchea/python-scraper.git
cd python-scraper
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## â–¶ï¸ Running the Scraper

```bash
python src/main.py
```

The output will be saved in `exported_data/` as a timestamped `.csv` file.

---

## ğŸ§ª Running Tests

Run all tests using:

```bash
pytest tests/
```

---

## âš ï¸ Important Note on Limitations

This scraper is implemented using `requests` and `BeautifulSoup`, which works for fetching public product data from Newegg under normal conditions. However, Newegg uses Cloudflare protection that may trigger a CAPTCHA challenge after several requests or depending on network behavior.

When this happens, the page content returned by requests is no longer the actual product page, but a Cloudflare challenge or CAPTCHA screen. This will cause the scraper to fail in collecting real product data.

The proper workaround in such cases involves using:

- `Selenium` with a real browser, which can render JavaScript and solve Cloudflare challenges.

- Rotating proxies or residential IPs to avoid rate-limiting and detection.

During development, I tested multiple free proxy options, but none were stable, fast, or reliable enough to maintain consistent scraping. All either timed out or failed HTTPS handshakes.

For production-grade scraping, integrating paid proxy services (such as ScraperAPI or Smartproxy) or browser automation tools (such as undetected-chromedriver) is highly recommended.

---

## âœ¨ Author

**Zojche Atanasova**  
[GitHub @zojchea](https://github.com/zojchea)

---
